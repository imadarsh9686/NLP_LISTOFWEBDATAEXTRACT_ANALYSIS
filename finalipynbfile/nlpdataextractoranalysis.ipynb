{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9780,
     "status": "ok",
     "timestamp": 1704473620143,
     "user": {
      "displayName": "senthil_kumar_ 21",
      "userId": "08386911802531083292"
     },
     "user_tz": -330
    },
    "id": "M2wdy4KbLsYf",
    "outputId": "7b0fd119-5f20-4acd-8b52-b148a025cc65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\senth\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: textblob in c:\\users\\senth\\anaconda3\\lib\\site-packages (0.17.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\senth\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\senth\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from spacy) (1.26.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: click in c:\\users\\senth\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\senth\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\senth\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\senth\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy textblob pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jyuNh8EbLoqD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30420,
     "status": "ok",
     "timestamp": 1704473679716,
     "user": {
      "displayName": "senthil_kumar_ 21",
      "userId": "08386911802531083292"
     },
     "user_tz": -330
    },
    "id": "19gmf7l0Mnux",
    "outputId": "b20e4967-15da-4387-e904-4d86486f7a62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83930,
     "status": "ok",
     "timestamp": 1704475587439,
     "user": {
      "displayName": "senthil_kumar_ 21",
      "userId": "08386911802531083292"
     },
     "user_tz": -330
    },
    "id": "zDbe6XZYL3MC",
    "outputId": "e3d0e406-99b3-4ec8-80f0-85f71f3fda43"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 111\u001b[0m\n\u001b[0;32m    109\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/senth/OneDrive/Desktop/New folder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Load spaCy model\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    112\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# Load Stop Words, Positive Words, and Negative Words\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m     52\u001b[0m         name,\n\u001b[0;32m     53\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m     54\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[0;32m     55\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[0;32m     56\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m     57\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     58\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "#from google.colab import files\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def load_stop_words(stop_words_folder):\n",
    "    stop_words = set()\n",
    "    for filename in os.listdir(stop_words_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(stop_words_folder, filename)\n",
    "            with open(file_path, 'r', encoding='latin-1') as file:\n",
    "                stop_words.update(set(file.read().splitlines()))\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def load_positive_negative_words(positive_words_path, negative_words_path):\n",
    "    with open(positive_words_path, 'r', encoding='ISO-8859-1') as file:\n",
    "        positive_words = set(file.read().splitlines())\n",
    "\n",
    "    with open(negative_words_path, 'r', encoding='ISO-8859-1') as file:\n",
    "        negative_words = set(file.read().splitlines())\n",
    "\n",
    "    return positive_words, negative_words\n",
    "\n",
    "\n",
    "def clean_text(article_text, stop_words):\n",
    "    cleaned_tokens = [word.lower() for word in word_tokenize(article_text) if word.isalpha() and word.lower() not in stop_words]\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "def analyze_text(article_text, stop_words, positive_words, negative_words):\n",
    "    cleaned_text = clean_text(article_text, stop_words)\n",
    "    cleaned_tokens = [word.lower() for word in word_tokenize(cleaned_text) if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "    sentiment_analysis = TextBlob(cleaned_text)\n",
    "    positive_score = sum(1 for word in sentiment_analysis.words if word in positive_words)\n",
    "    negative_score = sum(1 for word in sentiment_analysis.words if word in negative_words)\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / ((len(cleaned_text.split()) + 0.000001))\n",
    "\n",
    "    doc = nlp(article_text)\n",
    "    num_sentences = len(list(doc.sents))\n",
    "    avg_sentence_length = len(doc) / num_sentences\n",
    "    word_count = len(re.findall(r'\\b\\w+\\b', cleaned_text))\n",
    "    avg_words_per_sentence = word_count / num_sentences\n",
    "    complex_word_count = sum(1 for token in doc if token.is_alpha and len(token.text) > 2)\n",
    "    percentage_of_complex_words = (complex_word_count / word_count) * 100\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_of_complex_words)\n",
    "\n",
    "    syllable_per_word = syllable_count(cleaned_text) / word_count\n",
    "    personal_pronouns = count_personal_pronouns(doc)\n",
    "    avg_word_length = sum(len(word) for word in cleaned_tokens) / len(cleaned_tokens)\n",
    "\n",
    "    return [positive_score, negative_score, polarity_score, subjectivity_score,\n",
    "            avg_sentence_length, percentage_of_complex_words, fog_index,\n",
    "            avg_words_per_sentence, complex_word_count, word_count,\n",
    "            syllable_per_word, personal_pronouns, avg_word_length]\n",
    "\n",
    "\n",
    "def syllable_count(word):\n",
    "    # Simple syllable count, not perfect but can be used as an approximation\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "    for char in word:\n",
    "        if char.lower() in vowels:\n",
    "            count += 1\n",
    "    return max(count, 1)  # At least one syllable\n",
    "\n",
    "def count_personal_pronouns(doc):\n",
    "    personal_pronouns = ['I', 'me', 'my', 'mine', 'myself', 'you', 'your', 'yours', 'yourself', 'we', 'us', 'our', 'ours', 'ourselves']\n",
    "    return sum(1 for token in doc if token.text.lower() in personal_pronouns)\n",
    "\n",
    "# ... (Rest of the code remains unchanged)\n",
    "\n",
    "def extract_article(url, output_directory):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        title = soup.title.text.strip().replace('- Blackcoffer Insights', '')\n",
    "        article_paragraphs = soup.find_all('p')\n",
    "        article_text = ' '.join([p.text.strip() for p in article_paragraphs])\n",
    "\n",
    "        footer_text = \"Contact us: hello@blackcoffer.com © All Right Reserved, Blackcoffer(OPC) Pvt. Ltd\"\n",
    "        article_text = article_text.replace(footer_text, '')\n",
    "\n",
    "        full_output_path = os.path.join(output_directory, output_filename)\n",
    "        print(f\"Output Filename: {full_output_path}\")\n",
    "\n",
    "        with open(full_output_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(f\"{title}\\n\\n{article_text}\")\n",
    "\n",
    "        return title, article_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "\n",
    "# Set the working directory\n",
    "os.chdir(\"C:/Users/senth/OneDrive/Desktop/New folder\")\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load Stop Words, Positive Words, and Negative Words\n",
    "stop_words_folder = 'C:/Users/senth/OneDrive/Desktop/New folder/StopWords'\n",
    "positive_words_path = 'C:/Users/senth/OneDrive/Desktop/New folder/MasterDictionary/positive-words.txt'\n",
    "negative_words_path = 'C:/Users/senth/OneDrive/Desktop/New folder/MasterDictionary/negative-words.txt'\n",
    "stop_words = load_stop_words(stop_words_folder)\n",
    "positive_words, negative_words = load_positive_negative_words(positive_words_path, negative_words_path)\n",
    "\n",
    "# Read input data from Excel file\n",
    "df = pd.read_excel('C:/Users/senth/OneDrive/Desktop/New folder/Input.xlsx')\n",
    "\n",
    "# Create DataFrame for output\n",
    "output_directory = \"output_data\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "output_columns = pd.read_excel('C:/Users/senth/OneDrive/Desktop/New folder/Output Data Structure.xlsx', header=None, names=['Variable'])['Variable'].tolist()\n",
    "df_output = pd.DataFrame(columns=output_columns)\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    numeric_part = ''.join(filter(str.isdigit, row['URL_ID']))\n",
    "\n",
    "    if numeric_part:\n",
    "        url_id = int(numeric_part)\n",
    "        url = row['URL']\n",
    "\n",
    "        output_filename = f\"blackassign{url_id:04d}.txt\"\n",
    "        output_filepath = os.path.abspath(os.path.join(output_directory, output_filename))\n",
    "        os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "\n",
    "        title, article_text = extract_article(url, output_directory)\n",
    "\n",
    "        if article_text is not None:\n",
    "            analysis_results = analyze_text(article_text, stop_words, positive_words, negative_words)\n",
    "\n",
    "            new_row = {\n",
    "                'URL_ID': url_id,\n",
    "                'URL': url,\n",
    "                'POSITIVE SCORE': analysis_results[0],\n",
    "                'NEGATIVE SCORE': analysis_results[1],\n",
    "                'POLARITY SCORE': analysis_results[2],\n",
    "                'SUBJECTIVITY SCORE': analysis_results[3],\n",
    "                'AVG SENTENCE LENGTH': analysis_results[4],\n",
    "                'PERCENTAGE OF COMPLEX WORDS': analysis_results[5],\n",
    "                'FOG INDEX': analysis_results[6],\n",
    "                'AVG NUMBER OF WORDS PER SENTENCE': analysis_results[7],\n",
    "                'COMPLEX WORD COUNT': analysis_results[8],\n",
    "                'WORD COUNT': analysis_results[9],\n",
    "                'SYLLABLE PER WORD': analysis_results[10],\n",
    "                'PERSONAL PRONOUNS': analysis_results[11],\n",
    "                'AVG WORD LENGTH': analysis_results[12],\n",
    "            }\n",
    "\n",
    "            data_list.append(new_row)\n",
    "\n",
    "df_output = pd.DataFrame(data_list)\n",
    "\n",
    "output_file = 'Output Data Structure.xlsx'\n",
    "df_output.to_excel(output_file, index=False)\n",
    "print(f\"Textual analysis results saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 402,
     "status": "ok",
     "timestamp": 1704475986244,
     "user": {
      "displayName": "senthil_kumar_ 21",
      "userId": "08386911802531083292"
     },
     "user_tz": -330
    },
    "id": "0J_56tT_Yq32",
    "outputId": "d7d46acd-6178-4e56-d54d-a2a742ba9c2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result DataFrame:\n",
      "    URL_ID                                                URL  POSITIVE SCORE  \\\n",
      "0        1  https://insights.blackcoffer.com/rising-it-cit...              11   \n",
      "1        2  https://insights.blackcoffer.com/rising-it-cit...              60   \n",
      "2        3  https://insights.blackcoffer.com/internet-dema...              43   \n",
      "3        4  https://insights.blackcoffer.com/rise-of-cyber...              41   \n",
      "4        5  https://insights.blackcoffer.com/ott-platform-...              26   \n",
      "..     ...                                                ...             ...   \n",
      "93      96  https://insights.blackcoffer.com/what-is-the-r...              32   \n",
      "94      97  https://insights.blackcoffer.com/impact-of-cov...              26   \n",
      "95      98  https://insights.blackcoffer.com/contribution-...               5   \n",
      "96      99  https://insights.blackcoffer.com/how-covid-19-...              18   \n",
      "97     100  https://insights.blackcoffer.com/how-will-covi...              33   \n",
      "\n",
      "    NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
      "0                5        0.375000            0.057348            20.655172   \n",
      "1               35        0.263158            0.111765            23.100000   \n",
      "2               28        0.211268            0.099719            23.709677   \n",
      "3               79       -0.316667            0.169252            25.298246   \n",
      "4               12        0.368421            0.081023            23.511628   \n",
      "..             ...             ...                 ...                  ...   \n",
      "93              61       -0.311828            0.139640            29.180000   \n",
      "94              39       -0.200000            0.122642            36.512821   \n",
      "95               4        0.111111            0.042857            55.428571   \n",
      "96               7        0.440000            0.069638            26.062500   \n",
      "97              59       -0.282609            0.170686            36.771429   \n",
      "\n",
      "    PERCENTAGE OF COMPLEX WORDS  FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  \\\n",
      "0                    144.444444  66.039847                          9.620690   \n",
      "1                    154.705882  71.122353                         10.625000   \n",
      "2                    147.893258  68.641174                         11.483871   \n",
      "3                    143.300423  67.439467                         12.438596   \n",
      "4                    156.929638  72.176506                         10.906977   \n",
      "..                          ...        ...                               ...   \n",
      "93                   157.957958  74.855183                         13.320000   \n",
      "94                   190.943396  90.982487                         13.589744   \n",
      "95                   131.428571  74.742857                         30.000000   \n",
      "96                   165.738162  76.720265                         11.218750   \n",
      "97                   167.532468  81.721558                         15.400000   \n",
      "\n",
      "    COMPLEX WORD COUNT  WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  \\\n",
      "0                  403         279           2.677419                  5   \n",
      "1                 1315         850           2.996471                  9   \n",
      "2                 1053         712           3.213483                 21   \n",
      "3                 1016         709           3.149506                  8   \n",
      "4                  736         469           2.820896                 10   \n",
      "..                 ...         ...                ...                ...   \n",
      "93                1052         666           2.828829                  4   \n",
      "94                1012         530           2.737736                 14   \n",
      "95                 276         210           2.609524                  1   \n",
      "96                 595         359           2.640669                  5   \n",
      "97                 903         539           2.810761                  5   \n",
      "\n",
      "    AVG WORD LENGTH  \n",
      "0          6.591398  \n",
      "1          7.350588  \n",
      "2          8.015449  \n",
      "3          7.885755  \n",
      "4          7.217484  \n",
      "..              ...  \n",
      "93         7.150150  \n",
      "94         6.694340  \n",
      "95         6.671429  \n",
      "96         6.582173  \n",
      "97         7.055659  \n",
      "\n",
      "[98 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "df_result = pd.read_excel('/content/gdrive/MyDrive/NLP BLACKCOFFER PROJECT/Output Data Structure.xlsx')\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Result DataFrame:\")\n",
    "print(df_result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
